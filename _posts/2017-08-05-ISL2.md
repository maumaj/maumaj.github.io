---
layout: post
title: "Adventures in ISL (Pt. 2): Labs on Linear Regression"
date: 2017-08-05
categories: [Learning Resources]
---
In our continued journey through *Introduction to Statistical Learning*, we start using R to construct real, predictive machine learning models. Naturally, we start slow, with linear regression. Let's use the book's Lab to take a look at implementing chapter 3.


```R
#load new libraries to access other datasets
library(MASS)
library(ISLR)
```


```R
#explore variables in Boston dataset from MASS
names(Boston)
```


![png](http://i.imgur.com/wUQJbY3.png)





```R
#find relationship between median house value and percentage of households
#with low socioeconomic statuses
lm.fit = lm(medv ~ lstat, data=Boston)
lm.fit
```



    Call:
    lm(formula = medv ~ lstat, data = Boston)

    Coefficients:
    (Intercept)        lstat  
          34.55        -0.95  




Let's take a closer look at the details of this relationship --

Residuals: distance from the observed values to the values predicted by
our regression model

Coefficients: the line-of-best-fit, or predicted relationship, given by
linear regression; includes one-to-one dependency and predicted value at
X=0. Pr(>|t|) gives the p-value, the chance that the observed value is as
or more extreme than our predicted value. (*** indicates high significance)

RSE: square root of the mean square error
R-squared: proportion of observed values explained by our linear regression
model

```R
summary(lm.fit)
```



    Call:
    lm(formula = medv ~ lstat, data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max
    -15.168  -3.990  -1.318   2.034  24.500

    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 34.55384    0.56263   61.41   <2e-16 ***
    lstat       -0.95005    0.03873  -24.53   <2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 6.216 on 504 degrees of freedom
    Multiple R-squared:  0.5441,	Adjusted R-squared:  0.5432
    F-statistic: 601.6 on 1 and 504 DF,  p-value: < 2.2e-16




```R
#find out how the information is stored in our summary of lmfit
names(lm.fit)
```


![png](http://i.imgur.com/ZpD0bCO.png)





```R
#find the confidence interval
confint(lm.fit)
```


![png](http://i.imgur.com/fAaYR9e.png)




```R
#find the prediction interval for a given lsat value
predict(lm.fit, data.frame(lstat=(c(10,15,20))),interval="confidence")
predict(lm.fit, data.frame(lstat=(c(10,15,20))),interval="prediction")
```


![png](http://i.imgur.com/jJaT0o6.png)




```R
#check linearity of this relationship...
plot(Boston$lstat,Boston$medv, col="slateblue", xlab="Percentage of People with low Socioeconomic Status",
    ylab="Median House Value")
abline(lm.fit, col='purple', pch='+')
rect(1,35,10,51,border='red',lty='dotted',density=2,lwd=2)
legend(20,50,"lm fit: y = -0.95x + 34.55",bg='lavender',adj=0.075)
```


![png](http://i.imgur.com/LYsp3zI.png)


In some areas, this doesn't seem like a linear relationship -- see the outlined points in the upper left hand corner. Let's expound on this residual analysis to confirm that the relationship isn't linear. See the wonderful explanations at [the University of Virginia](http://data.library.virginia.edu/diagnostic-plots/) for some brief background.


```R
#break lm. fit into a few different charts
par(mfrow=c(2,2))
plot(lm.fit, col='pink')
```


![png](http://i.imgur.com/r3Qf6EG.png)



```R
#compute more detailed leverage statistics
plot(hatvalues(lm.fit), col='red',ylab='Leverage',main='Observation vs Leverage')
which.max(hatvalues(lm.fit)) #which observation has the largest leverage statistic? (at about Cook's Distance above)
```


<strong>375:</strong> 375



![png](http://i.imgur.com/Dpah1e1.png)


It seems that simple linear regression isn't making the cut. Let's move on to multiple linear regression -- how does our model improve by incorporating the average age of houses (age)?


```R
lm.fit = lm(Boston$medv ~ Boston$lstat + Boston$age)
summary(lm.fit)
```



    Call:
    lm(formula = Boston$medv ~ Boston$lstat + Boston$age)

    Residuals:
        Min      1Q  Median      3Q     Max
    -15.981  -3.978  -1.283   1.968  23.158

    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  33.22276    0.73085  45.458  < 2e-16 ***
    Boston$lstat -1.03207    0.04819 -21.416  < 2e-16 ***
    Boston$age    0.03454    0.01223   2.826  0.00491 **
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 6.173 on 503 degrees of freedom
    Multiple R-squared:  0.5513,	Adjusted R-squared:  0.5495
    F-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16



While our F-statistic has tanked, our p-value remains constant; both R-squared and RSE have improved. Looks great thus far. What if we incorporate all predictors into our model?


```R
lm.fit = lm(Boston$medv~., data=Boston)
summary(lm.fit)
```



    Call:
    lm(formula = Boston$medv ~ ., data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max
    -15.595  -2.730  -0.518   1.777  26.199

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
    crim        -1.080e-01  3.286e-02  -3.287 0.001087 **
    zn           4.642e-02  1.373e-02   3.382 0.000778 ***
    indus        2.056e-02  6.150e-02   0.334 0.738288    
    chas         2.687e+00  8.616e-01   3.118 0.001925 **
    nox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
    rm           3.810e+00  4.179e-01   9.116  < 2e-16 ***
    age          6.922e-04  1.321e-02   0.052 0.958229    
    dis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
    rad          3.060e-01  6.635e-02   4.613 5.07e-06 ***
    tax         -1.233e-02  3.760e-03  -3.280 0.001112 **
    ptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
    black        9.312e-03  2.686e-03   3.467 0.000573 ***
    lstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 4.745 on 492 degrees of freedom
    Multiple R-squared:  0.7406,	Adjusted R-squared:  0.7338
    F-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16



Moving on up! Yes, our F-statistic dropped again, but with RSE and R-square on the rise and a constant p-value, I'm happy. Look at those great p-values! (aside from that for 'age' -- so let's drop that)


```R
lm.fit1 = lm(Boston$medv~. -age, data=Boston)
summary(lm.fit1)
```



    Call:
    lm(formula = Boston$medv ~ . - age, data = Boston)

    Residuals:
         Min       1Q   Median       3Q      Max
    -15.6054  -2.7313  -0.5188   1.7601  26.2243

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  36.436927   5.080119   7.172 2.72e-12 ***
    crim         -0.108006   0.032832  -3.290 0.001075 **
    zn            0.046334   0.013613   3.404 0.000719 ***
    indus         0.020562   0.061433   0.335 0.737989    
    chas          2.689026   0.859598   3.128 0.001863 **
    nox         -17.713540   3.679308  -4.814 1.97e-06 ***
    rm            3.814394   0.408480   9.338  < 2e-16 ***
    dis          -1.478612   0.190611  -7.757 5.03e-14 ***
    rad           0.305786   0.066089   4.627 4.75e-06 ***
    tax          -0.012329   0.003755  -3.283 0.001099 **
    ptratio      -0.952211   0.130294  -7.308 1.10e-12 ***
    black         0.009321   0.002678   3.481 0.000544 ***
    lstat        -0.523852   0.047625 -10.999  < 2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 4.74 on 493 degrees of freedom
    Multiple R-squared:  0.7406,	Adjusted R-squared:  0.7343
    F-statistic: 117.3 on 12 and 493 DF,  p-value: < 2.2e-16



But it's not that simple. Many of these predictors influence one another, and can be labeled *interaction terms*. It's time to account for the relationships between our predictors.


```R
summary(lm(Boston$medv ~ Boston$lstat * Boston$age, data=Boston))
```



    Call:
    lm(formula = Boston$medv ~ Boston$lstat * Boston$age, data = Boston)

    Residuals:
        Min      1Q  Median      3Q     Max
    -15.806  -4.045  -1.333   2.085  27.552

    Coefficients:
                              Estimate Std. Error t value Pr(>|t|)    
    (Intercept)             36.0885359  1.4698355  24.553  < 2e-16 ***
    Boston$lstat            -1.3921168  0.1674555  -8.313 8.78e-16 ***
    Boston$age              -0.0007209  0.0198792  -0.036   0.9711    
    Boston$lstat:Boston$age  0.0041560  0.0018518   2.244   0.0252 *  
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 6.149 on 502 degrees of freedom
    Multiple R-squared:  0.5557,	Adjusted R-squared:  0.5531
    F-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16



In this model, incorporating interaction terms leads to a step backwards. Nevertheless, the concept is important to retain and practice. Another feature of linear regression is a non-linear transformation. That is, adding a p<sup>2</sup> or p<sup>z</sup> to our equation.


```R
lm.fit2 = lm(medv ~ lstat + I(lstat^2), data=Boston)
summary(lm.fit2)
```



    Call:
    lm(formula = medv ~ lstat + I(lstat^2), data = Boston)

    Residuals:
         Min       1Q   Median       3Q      Max
    -15.2834  -3.8313  -0.5295   2.3095  25.4148

    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 42.862007   0.872084   49.15   <2e-16 ***
    lstat       -2.332821   0.123803  -18.84   <2e-16 ***
    I(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 5.524 on 503 degrees of freedom
    Multiple R-squared:  0.6407,	Adjusted R-squared:  0.6393
    F-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16




```R
#run an ANOVA test to run a hypothesis test -- does the non-linear model provide a better fit?
lm.fit = lm(medv~lstat, data=Boston)
anova(lm.fit, lm.fit2)
```


![png](http://i.imgur.com/3JhPea0.png)





```R
#check for patterns in the residual
par(mfrow=c(2,2))
plot(lm.fit2, col='purple')
```


![png](http://i.imgur.com/LjRun5O.png)


Looking quite a bit better compared to our linear model. How do we scale equation-derivation to use greater polynomials? With the poly() function!


```R
lm.fit5 = lm(medv ~ poly(lstat,5), data=Boston)
summary(lm.fit5)
```



    Call:
    lm(formula = medv ~ poly(lstat, 5), data = Boston)

    Residuals:
         Min       1Q   Median       3Q      Max
    -13.5433  -3.1039  -0.7052   2.0844  27.1153

    Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
    (Intercept)       22.5328     0.2318  97.197  < 2e-16 ***
    poly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***
    poly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***
    poly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***
    poly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***
    poly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 5.215 on 500 degrees of freedom
    Multiple R-squared:  0.6817,	Adjusted R-squared:  0.6785
    F-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16



I've now worked through the chapter three lab, but there's plenty more to learn about linear regression. Check back soon to see how I approach ISL's exercises.
