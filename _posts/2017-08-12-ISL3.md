---
layout: post
title: "Adventures in ISL (Pt. 3): Exercising Regression"
date: 2017-08-12
categories: [Learning Resources]
---
It's time to jump back into *Introduction to Statistical Exercises*, and begin working through those chapter three exercises. Here we use linear regression models to investigate predictors of cars' mpg.


```R
auto <- read.csv('Auto.csv')
tail(auto,10)
names(auto)
str(auto)
auto$horsepower <- as.numeric(auto$horsepower)
```


![png](http://i.imgur.com/ypy994o.png)

![png](http://i.imgur.com/KUDkSlP.png)



```R
#run simple linear regression to find the relationship between mpg and horsepower, then summarize
lm.fit <- lm(auto$mpg~auto$horsepower)
summary(lm.fit)
```

![png](http://i.imgur.com/0EjcSYv.png)


Although our p-value does indicate that there is a linear relationship between miles per gallon and horsepower, we see a high RSE and a low R-squared -- there may be a relationship, but our linear model scarcely describes it. Let's make a visual confirmation.


```R
plot(auto$horsepower, auto$mpg, col="violet")
abline(lm.fit, col="red")
```


![png](http://i.imgur.com/TQAxh5P.png)


As suspected, this simple linear model doesn't fully describe the relationship in question. There seem to be two clusters, indicating that cars have either high mpg and horsepower or low mpg and horsepower. Perhaps we can learn more by studying diagnostic plots.


```R
par(mfrow=(c(2,2)))
plot(lm.fit, col='blue')
```


![png](http://i.imgur.com/i1Wnbds.png)


With the help of our residuals vs. plotted chart, we notice a clear pattern. It seems like linear regression won't do the trick, so it's time to expand into multiple regression.


```R
pairs(auto, col=c("maroon"))
```


![png](http://i.imgur.com/WfwGhnw.png)



```R
## produce a matrix of correlations amongst the variables
Auto$name <- as.numeric(Auto$name)
cor(Auto)
Auto$name <- as.character(Auto$name)
str(Auto)
```


```R
#excluding 'name', let's perform a multiple linear regression
lm.fit2 = lm(Auto$mpg ~ . -name, data=Auto)
summary(lm.fit2)
```

![png](http://i.imgur.com/0vDPoOg.png)


The importance of multiple regression seems clear, with RSE cut in half and R-squared increasing nearly sixfold. There's now a clear relationship between the predictors and response, with weight, year, and origin impacting our model the most. In fact, every year mpg has increased by 0.750773. Again, visuals will help us ascertain the relationship.


```R
par(mfrow=(c(2,2)))
plot(lm.fit2, col='navy')
```


![png](http://i.imgur.com/6cMYyKs.png)


Well, so much for our perfect model. Our residual plot displays a bowed shape, and residuals jump off track in the Normal-QQ plot. To make matters worse, we see one point with extreme leverage. Despite its shortfalls, the model's a good starting point for predicting mpg.

Will any interaction effects or transformations help our case?


```R
lm.fit3 = lm(mpg~. -name + weight*displacement + year*acceleration, data=auto)
summary(lm.fit3)
```



![png](http://i.imgur.com/EHj3sCo.png)



Clearly, incorporating interaction effects can improve the accuracy of our model, lessening RSE and raising R-squared. But are the problems with residuals exacerterbated?


```R
par(mfrow=c(2,2),col='darkslategray4')
plot(lm.fit3)
```


![png](http://i.imgur.com/VyFkoFA.png)


Although the Scale-Location chart displays a steeper slope, our Residuals vs. fitted chart shows improvement.

Finally, we can take a look at transforming our predictors to improve this machine learning model.


```R
lm.fit4 = lm(mpg~. -name + weight*displacement + year*acceleration
             + I(1/displacement^2) + I(1/weight^2), data=auto)
summary(lm.fit4)
```


![png](http://i.imgur.com/n09Rze0.png)



Yep, we can move our model a bit forward with non-linear transformations! For the final time, let's check the residuals.


```R
par(mfrow=(c(2,2)), col='slateblue4')
plot(lm.fit4)
```


![png](http://i.imgur.com/Y8P8Te2.png)


These litmus tests seem unchanged, informing us that model improvements have not been compromised. But wait! There's so much more. In the next post, we'll investigate how qualatative variables are incorporated into regression models.
