---
layout: post
title: "Where do they graduate? (Pt. 2)"
date: 2018-09-24
categories: [Explorations]
---

In last week's post, we explored how we can use data from College Scorecard to find predictors of college graduation rates. But will these college statistics actually help us predict how many students get a diploma? Let's use linear regression (and some variants) to find out.


```python
#the setup -- loading libraries for analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from math import *
plt.style.use('bmh')
%matplotlib inline
```


```python
#load college scorecard data
college = pd.read_csv('college14_15.csv',low_memory=False)
college.tail()
```

![png](http://i.imgur.com/XkJUok6.png)

```python
#clean up the dataset -- remove values that have been suppressed for privacy
clean_college = college.copy()
clean_college['AGE_ENTRY'] = (clean_college.AGE_ENTRY.apply(pd.to_numeric, errors='coerce'))
clean_college['FEMALE'] = (clean_college.FEMALE.apply(pd.to_numeric, errors='coerce'))
clean_college['MARRIED'] = (clean_college.MARRIED.apply(pd.to_numeric, errors='coerce'))
clean_college['DEPENDENT'] = (clean_college.DEPENDENT.apply(pd.to_numeric, errors='coerce'))
clean_college['VETERAN'] = (clean_college.VETERAN.apply(pd.to_numeric, errors='coerce'))
clean_college['FIRST_GEN'] = (clean_college.FIRST_GEN.apply(pd.to_numeric, errors='coerce'))
clean_college['DEP_INC_AVG'] = (clean_college.DEP_INC_AVG.apply(pd.to_numeric, errors='coerce'))
clean_college = clean_college.fillna(clean_college.mean())
```

Let's use a classic linear model that minimizes the squares of regressions from observed graduation rates. We'll take thirteen features investigated in the first part of this post. Using the linear model fitted by scikit-learn, let's take a look at potential coefficients. These quantify the relationship between our predictive features and the response variable, graduation rate.


```python
from sklearn import linear_model
reg = linear_model.LinearRegression()
college_pred_indices = ['SAT_AVG','ACTCMMID','TUITIONFEE_OUT','INEXPFTE','AVGFACSAL','PFTFAC',
                                   'DEP_INC_AVG','AGE_ENTRY','MARRIED','DEPENDENT','VETERAN','FIRST_GEN','ADM_RATE']
college_predictors = clean_college[college_pred_indices]
grad_rate = clean_college['C150_4']
reg.fit(college_predictors, grad_rate)
reg.coef_
```

![png](http://i.imgur.com/Bi2JZCn.png)



With that, we can evaluate our model. Mean-squared-error looks at the difference between predicted values and observed values, squares each, and takes the mean. Roughly speaking, our R-squared value shows the proportion of y-values ('graduation rates') explained by our predictors.

Ok, how does our model stack up?


```python
#MSE
np.mean((reg.predict(college_predictors) - grad_rate) ** 2)
```




    0.009053284992404074




```python
#R-squared
from sklearn import metrics
metrics.r2_score(grad_rate, reg.predict(college_predictors), sample_weight=None, multioutput=None)
```




    0.38469186752187723



Not bad. But notice -- we've trained our model on the same data that we've tested it on. Therefore, we might've *overfit* our model and increased its *variability*. That is, our model might fit the training data too closely, and fall apart when it comes to new data. With k-fold cross validation, we'll curb overfitting by (1) splitting the data into *k* sets (2) training on one data subset, testing on the rest (3) repeat step 2 with the remaining subsets of data. We'll then take the average to see how our model holds up to different sets of colleges -- can it be generalized?


```python
from sklearn.model_selection import KFold
cv = pd.DataFrame(data={
    'MSE': [],
    'R-squared': []
})

kf = KFold(n_splits=5)
for train, test in kf.split(college_predictors):
    x = np.array(college_predictors)
    y = np.array(grad_rate)
    x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]
    reg.fit(x_train,y_train)
    mse = (np.mean((reg.predict(x_test) - y_test) ** 2))
    rsq = metrics.r2_score(y_test, reg.predict(x_test), sample_weight=None, multioutput=None)
    cv = cv.append({
        'MSE': mse,
        'R-squared': rsq,
    }, ignore_index=True)

cv.mean()
```

![png](http://i.imgur.com/l6Eoiyj.png)


Our MSE has increased and R-squared risen -- but that's to be expected! With a mean-squared error of less than 1%, our model still has some predictive power. Can we improve it? You bet!

For one, not every predictor may be contributing to the model's success. Although we could use step-forward, step-backward, or a combination thereof to pick predictors, with only thirteen predictive features I'll just power through with brute force. Power, of course, because I'll use a power set. That'll give me all combinations of possible predictors (excluding the empty set). From there, we can iterate through each subset of predictors, use k-fold cross validation on a simple linear regression model, and use the most successful group of predictors.


```python
def powerset(seq):
    if len(seq) <= 1:
        yield seq
    else:
        for item in powerset(seq[1:]):
            yield [seq[0]]+item
            yield item
```


```python
predictors = [x for x in powerset(college_pred_indices)]
predictors.sort()
predictors
```
![png](http://i.imgur.com/7Lc0GPU.png)

```python
def brutePredictorSelection(predictors, response, df):
    brute_cv = pd.DataFrame(data = {
        'MSE': [],
        'R-squared': [],
        'Predictors': []
    })

    predictorPower = [x for x in powerset(predictors)]
    predictorPower.sort()

    for predictorArray in predictorPower:
        cv = pd.DataFrame(data = {
            'MSE': [],
            'R-squared': []
        })

        kF = KFold(n_splits=10)
        for train, test in kf.split(df[predictorArray]):
            x = np.array(df[predictorArray])
            y = np.array(df[response])

            x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]

            reg = linear_model.LinearRegression()
            reg.fit(x_train,y_train)

            mse = (np.mean((reg.predict(x_test) - y_test) ** 2))
            rsq = metrics.r2_score(y_test, reg.predict(x_test), sample_weight=None, multioutput=None)

            cv = cv.append({
                'MSE': mse,
                'R-squared': rsq,
            }, ignore_index=True)

        brute_cv = brute_cv.append({
            'MSE': cv['MSE'].mean(),
            'R-squared': cv['R-squared'].mean(),
            'Predictors': predictorArray,
        }, ignore_index=True)

    return brute_cv

```


```python
brute_cv = brutePredictorSelection(college_pred_indices,'C150_4',clean_college)
```


```python
brute_cv = brute_cv.sort_values('R-squared',ascending=False)
brute_cv.head()
```

![png](http://i.imgur.com/RfE2tnL.png)


```python
brute_cv.loc[3166]['Predictors']
```

![png](http://i.imgur.com/DeaaIUG.png)

Great! We've found the most powerful predictors of a six-year graduation rate, a subset of seven. This technique doesn't lower MSE significantly, but it's a move in the right direction. Instead of simply picking the best predictors, we can also weight them using the *Ridge* method. Normally, we might also choose to implement the *Lasso* regression method, which adjusts some of the predictors to 0. Since we're iterating through a power set, however, we've already effectively weighted some predictors at 0.


```python
def brutePredictorRidge(predictors, response, df, alphaList):
    brute_cv = pd.DataFrame(data = {
        'MSE': [],
        'R-squared': [],
        'Predictors': []
    })

    predictorPower = [x for x in powerset(predictors)]
    predictorPower.sort()

    for predictorArray in predictorPower:
        cv = pd.DataFrame(data = {
            'MSE': [],
            'R-squared': []
        })

        kF = KFold(n_splits=10)
        for train, test in kf.split(df[predictorArray]):
            x = np.array(df[predictorArray])
            y = np.array(df[response])

            x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]

            reg = linear_model.RidgeCV(alphas=alphaList)
            reg.fit(x_train,y_train)

            mse = (np.mean((reg.predict(x_test) - y_test) ** 2))
            rsq = metrics.r2_score(y_test, reg.predict(x_test), sample_weight=None, multioutput=None)

            cv = cv.append({
                'MSE': mse,
                'R-squared': rsq,
            }, ignore_index=True)

        brute_cv = brute_cv.append({
            'MSE': cv['MSE'].mean(),
            'R-squared': cv['R-squared'].mean(),
            'Predictors': predictorArray,
        }, ignore_index=True)

    return brute_cv
```


```python
brute_cv_ridge = brutePredictorRidge(college_pred_indices,'C150_4',clean_college,[0.0001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0])
brute_cv_ridge = brute_cv_ridge.sort_values('R-squared',ascending=False)
brute_cv_ridge.head()
```

![png](http://i.imgur.com/hL6MedV.png)


Seems like there's still room to grow! We can turn to *multiple linear regression* by transforming our predictors with a chosen exponential degree (here we chose a quadratic polynomial). Since we're using a transformation rather than plugging manipulated variables (p<sup>2</sup>, p<sup>3</sup>, etc), it's still a linear model. This allows us more flexibility -- but here it's doubly important to cross-validate lest we overfit.


```python
#multiple regression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

college_pred_indices = ['SAT_AVG','TUITIONFEE_OUT','AVGFACSAL','PFTFAC',
                                   'DEPENDENT','VETERAN','ADM_RATE']
poly = PolynomialFeatures(degree=4)
pred_t = poly.fit_transform(clean_college[college_pred_indices])
```


```python
cv = pd.DataFrame(data = {
            'MSE': [],
            'R-squared': []
        })

kF = KFold(n_splits=10)
for train, test in kf.split(pred_t):
    x = np.array(pred_t)
    y = np.array(clean_college['C150_4'])

    x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]

    reg = linear_model.LassoLarsCV()
    reg.fit(x_train,y_train)

    mse = (np.mean((reg.predict(x_test) - y_test) ** 2))
    rsq = metrics.r2_score(y_test, reg.predict(x_test), sample_weight=None, multioutput=None)

    print(reg.coef_)

    cv = cv.append({
        'MSE': mse,
        'R-squared': rsq,
    }, ignore_index=True)
```

![png](http://i.imgur.com/i6l6eOd.png)


```python
cv.mean()
```

![png](http://i.imgur.com/gpwV3nr.png)


With the use of multiple regression and the lasso method, we have a noticeable reduction in MSE and increase in R-squared. We also have a preview of the coefficients associated with each predictor, although with an R-squared of only 0.335 they cannot be interpreted as inferential. Still, we've learned that average SAT, out-of-state tuition, average faculty salary, share of full-time faculty, the average family income of dependent students, the share of veterans, and admission rate can be used as predictors for six-year graduation rate.

Still, there's plenty to glean from this college scorecard. We'll continue to approach the data with EDA and machine learning techniques, forming a more complete picture of trends in higher education.
